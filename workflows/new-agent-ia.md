---
description: Workflow para crear o mejorar un agente de IA
level: 2-3
personas: [ai-agent-engineer, backend-engineer]
version: 2.1
type: workflow
---

# New AI Agent Workflow

> **Tiempo estimado**: 4-8 horas | **Level**: 2-3

Este workflow gu√≠a la creaci√≥n o mejora de un agente de IA.

## Pre-requisitos

1. Leer [AGENTS.md](../AGENTS.md)
2. Leer [rules/agents-ia.md](../rules/agents-ia.md)
3. Leer [personas/ai-agent-engineer.md](../personas/ai-agent-engineer.md)

## Informaci√≥n Requerida

Antes de comenzar, necesito entender:

1. **Prop√≥sito del agente**: ¬øQu√© tarea debe realizar?
2. **Usuario objetivo**: ¬øQui√©n lo usar√°? (humano, n8n, otro sistema)
3. **Herramientas necesarias**: ¬øQu√© acciones debe poder ejecutar?
4. **Conocimiento requerido**: ¬øQu√© contexto/datos necesita?
5. **L√≠mites**: ¬øQu√© NO debe poder hacer?
6. **Modelo preferido**: ¬øGPT-4, Claude, Gemini, etc.?
7. **Presupuesto por ejecuci√≥n**: ¬øL√≠mite de costo?

---

## Paso 1: Dise√±ar el Agente

### 1.1 Definir Arquitectura

```
¬øEl agente necesita...?

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Usar herramientas‚îÇ‚îÄ‚îÄ‚ñ∂ Tool-based Agent (ReAct)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ M√∫ltiples pasos  ‚îÇ‚îÄ‚îÄ‚ñ∂ Multi-step Agent
‚îÇ de razonamiento  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Coordinar otros  ‚îÇ‚îÄ‚îÄ‚ñ∂ Orchestrator Agent
‚îÇ agentes          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Solo responder   ‚îÇ‚îÄ‚îÄ‚ñ∂ Simple LLM Chain
‚îÇ preguntas        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Definir Herramientas

| Tool | Prop√≥sito | Par√°metros |
|------|-----------|------------|
| [tool_name] | [descripci√≥n] | [param1, param2, ...] |
| [tool_name] | [descripci√≥n] | [param1, param2, ...] |

### 1.3 Definir Prompt del Sistema

```markdown
## Role
[Descripci√≥n del rol del agente]

## Available Tools
[Lista de herramientas con descripciones]

## Guidelines
[Instrucciones de c√≥mo debe comportarse]

## Output Format
[Formato esperado de respuestas]

## Constraints
[L√≠mites y restricciones]
```

---

## Paso 2: Plan de Implementaci√≥n

### Archivos a Crear

```markdown
## Configuraci√≥n
- [ ] `agents/config/agents.yaml` - Registrar agente

## Prompts
- [ ] `agents/python/prompts/personas/{agent_name}.md` - System prompt

## Herramientas (si son nuevas)
- [ ] `agents/python/tools/{tool_name}_tool.py` - Implementaci√≥n
- [ ] `agents/config/tools.yaml` - Registrar tool

## Tests
- [ ] `agents/python/tests/test_{agent_name}.py` - Tests del agente

## API (si se expone via HTTP)
- [ ] `app/routers/agents/{agent_name}.py` - Endpoint
```

---

## Paso 3: Implementar Herramientas

### 3.1 Crear Tool (si es nueva)

```python
# agents/python/tools/{tool_name}_tool.py

from lmagent.tools.base import BaseTool, ToolResult
from pydantic import Field
import structlog

logger = structlog.get_logger()

class {ToolName}Tool(BaseTool):
    """
    {Descripci√≥n de la herramienta}.
    
    Usa esta herramienta cuando necesites:
    - [caso de uso 1]
    - [caso de uso 2]
    
    NO uses para:
    - [caso donde NO aplica]
    """
    
    name: str = "{tool_name}"
    description: str = "{Descripci√≥n corta para el LLM}"
    
    async def execute(
        self,
        param1: str = Field(..., description="Descripci√≥n del par√°metro"),
        param2: int = Field(10, description="Par√°metro opcional con default")
    ) -> ToolResult:
        """Ejecuta la herramienta."""
        logger.info(
            "{tool_name}_execute",
            param1=param1,
            param2=param2
        )
        
        try:
            # Implementar l√≥gica
            result = await self._do_work(param1, param2)
            
            return ToolResult(
                success=True,
                data=result,
                metadata={"param1": param1}
            )
            
        except Exception as e:
            logger.error("{tool_name}_error", error=str(e))
            return ToolResult(
                success=False,
                error=str(e),
                metadata={"suggestion": "Sugerencia para el agente"}
            )
```

### 3.2 Registrar en tools.yaml

```yaml
# agents/config/tools.yaml

tools:
  {tool_name}:
    name: "{Tool Name}"
    module: "agents.python.tools.{tool_name}_tool"
    class: "{ToolName}Tool"
    description: "{Descripci√≥n}"
    category: "{category}"
    parameters:
      - name: param1
        type: string
        required: true
        description: "Descripci√≥n"
      - name: param2
        type: integer
        default: 10
        description: "Descripci√≥n"
```

---

## Paso 4: Crear el Prompt
> üí° **Tip**: P√≠dele al `/prompt` engineer que redacte esto por ti.

### 4.1 Escribir System Prompt
El path est√°ndar es `agents/python/prompts/personas/{agent_name}.md`.

```markdown
# agents/python/prompts/personas/{agent_name}.md

You are {Agent Name}, an AI assistant specialized in {domain}.

## Your Role
{Descripci√≥n detallada del rol}

## Available Tools

You have access to the following tools:

### {tool_1_name}
{Descripci√≥n de cu√°ndo y c√≥mo usar}

### {tool_2_name}
{Descripci√≥n de cu√°ndo y c√≥mo usar}

## How to Use Tools

When you need to take an action:
1. Think about which tool is most appropriate
2. Call the tool with the required parameters
3. Wait for the result
4. Use the result to inform your next step or response

## Response Guidelines

- Be concise and direct
- When using data from tools, cite the source
- If a tool fails, explain what happened and try an alternative
- If you cannot complete a task, explain why

## Constraints

- Maximum cost per session: ${max_cost}
- Maximum iterations: {max_iterations}
- Do not {restriction_1}
- Do not {restriction_2}

## Examples

### Example 1: {scenario}

User: "{user_input}"

Thinking: {thought_process}
Action: {tool_name}({params})
Observation: {result}
Response: "{final_response}"
```

---

## Paso 5: Configurar el Agente

### 5.1 Registrar en agents.yaml

```yaml
# agents/config/agents.yaml

agents:
  {agent_name}:
    name: "{Agent Display Name}"
    description: "{Descripci√≥n corta}"
    
    # LLM Configuration
    model: "gpt-4o"  # o claude-sonnet-4, gemini-2.5-pro
    temperature: 0.7
    max_tokens: 4096
    
    # Agent Configuration
    max_iterations: 10
    max_cost: 2.00
    
    # System Prompt
    system_prompt: "prompts/personas/{agent_name}.md"
    
    # Tools
    tools:
      - http_request
      - database_query
      - {custom_tool}
    
    # Guardrails
    guardrails:
      require_confirmation: ["database_write", "send_email"]
      blocked_actions: ["delete_production"]
```

---

## Paso 6: Exponer via API (Opcional)

```python
# app/routers/agents/{agent_name}.py

from fastapi import APIRouter
from pydantic import BaseModel

router = APIRouter(prefix="/agents/{agent-name}", tags=["agents"])

class AgentRequest(BaseModel):
    input: str
    context: dict = {}
    max_cost: float = 2.00

class AgentResponse(BaseModel):
    success: bool
    output: str
    cost: float
    iterations: int
    trajectory_id: str | None = None

@router.post("/run")
async def run_agent(request: AgentRequest) -> AgentResponse:
    """
    Ejecuta el agente {agent_name}.
    
    ## Uso en n8n
    
    1. HTTP Request node
    2. POST {{$env.BACKEND_URL}}/agents/{agent-name}/run
    3. Body: {"input": "tu pregunta o tarea"}
    """
    agent = await load_agent("{agent_name}")
    agent.cost_tracker.max_cost = request.max_cost
    
    result = await agent.run(
        user_input=request.input,
        context=request.context
    )
    
    return AgentResponse(
        success=True,
        output=result,
        cost=agent.cost_tracker.total,
        iterations=agent.iteration_count,
        trajectory_id=agent.trajectory_id
    )
```

---

## Paso 7: Testing

### 7.1 Tests del Agente

```python
# agents/python/tests/test_{agent_name}.py

import pytest
from unittest.mock import AsyncMock, patch

class TestAgent{AgentName}:
    """Tests para {agent_name}."""
    
    @pytest.mark.asyncio
    async def test_basic_query(self, agent):
        """Debe responder a query b√°sica."""
        result = await agent.run("Test query")
        assert result is not None
        assert len(result) > 0
    
    @pytest.mark.asyncio
    async def test_uses_correct_tool(self, agent, mock_llm):
        """Debe usar la herramienta correcta."""
        mock_llm.return_value = MockResponse(
            tool_calls=[{"name": "{tool_name}", "args": {...}}]
        )
        
        with patch.object(agent.tools['{tool_name}'], 'execute') as mock_tool:
            mock_tool.return_value = ToolResult(success=True, data={})
            await agent.run("Query that requires {tool_name}")
            mock_tool.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_handles_tool_error(self, agent):
        """Debe manejar errores de tools."""
        with patch.object(agent.tools['{tool_name}'], 'execute') as mock_tool:
            mock_tool.return_value = ToolResult(
                success=False,
                error="Simulated error"
            )
            result = await agent.run("Query")
            # Deber√≠a manejar el error gracefully
            assert "error" in result.lower() or agent.iteration_count > 1
    
    @pytest.mark.asyncio
    async def test_respects_cost_limit(self, agent):
        """Debe respetar l√≠mite de costo."""
        agent.cost_tracker.max_cost = 0.001  # Muy bajo
        
        with pytest.raises(CostLimitExceeded):
            await agent.run("Expensive query")
```

### 7.2 Test Manual

1. Ejecutar agente con queries de ejemplo
2. Verificar que usa tools correctamente
3. Verificar manejo de errores
4. Verificar l√≠mites de costo

---

## Paso 8: Documentaci√≥n

### Documentar en README del agente

```markdown
# {Agent Name}

## Descripci√≥n
{Qu√© hace este agente}

## Uso

### Via Python
```python
from agents.python.runner import load_agent

agent = await load_agent("{agent_name}")
result = await agent.run("Tu pregunta o tarea")
```

### Via API
```bash
curl -X POST {backend_url}/agents/{agent-name}/run \
  -H "Content-Type: application/json" \
  -d '{"input": "Tu pregunta o tarea"}'
```

### Via n8n
[Instrucciones de configuraci√≥n]

## Herramientas Disponibles
| Tool | Descripci√≥n |
|------|-------------|
| {tool_1} | {descripci√≥n} |
| {tool_2} | {descripci√≥n} |

## Ejemplos
[Ejemplos de uso con inputs y outputs]

## L√≠mites
- Costo m√°ximo: ${max_cost}
- Iteraciones m√°ximas: {max_iterations}
```

---

## Checklist Final

### Implementaci√≥n
- [ ] Prompt del sistema escrito y probado
- [ ] Herramientas implementadas (si nuevas)
- [ ] Agente registrado en agents.yaml
- [ ] API endpoint creado (si aplica)

### Testing
- [ ] Tests unitarios pasando
- [ ] Test manual con casos de uso reales
- [ ] Verificaci√≥n de l√≠mites de costo

### Documentaci√≥n
- [ ] README del agente
- [ ] Ejemplos de uso
- [ ] Documentaci√≥n para n8n (si aplica)

### Seguridad
- [ ] Guardrails configurados
- [ ] Permisos de tools revisados
- [ ] Logging de auditor√≠a habilitado

---

## üõ†Ô∏è Herramientas Sugeridas

| Fase | Herramienta |
|------|-------------|
| Dise√±o | `write_to_file` (prompts, configs) |
| Implementaci√≥n | `run_command` (tests), `view_file` |
| Testing | `run_command` (evals), `browser_subagent` |
| Docs | `mcp_context7_query-docs` (LangChain, etc.) |

## ‚ö†Ô∏è Errores Comunes

| Error | Soluci√≥n |
|-------|----------|
| Tools sin schema estricto | SIEMPRE definir con Pydantic/Field |
| Sin limite de costo | Configurar `max_cost` desde el inicio |
| Prompt vago | Ser espec√≠fico en rol, tools y constraints |
| Sin evals | Implementar tests de Faithfulness y Tool Accuracy |
